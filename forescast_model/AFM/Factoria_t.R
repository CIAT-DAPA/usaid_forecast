### Paquetes Necesarios
library(FactoMineR)
library(ggplot2)
library(raster) 
library(cowplot)
library(ade4)
library(grid)
library(nortest)
library(lmtest)
library(rasterVis)
library(gridExtra)


### Seleecion del directorio de trabajo
setwd("C:/Users/AESQUIVEL/Google Drive/salidas_afm/")
ruta=getwd() ##  Almacene la ruta de trabajo 

### Transformar un archivo .tsv(CPT) en raster 

# Función que permite crear un raster a partir de los valores 
# de la tabla depurados, (esta función esta programada para los datos de la TSM observada)
transform_raster=function(x){ 
  # Primero se crea un raster teniendo encuenta la resolución espacial de la tabla .tsv  
  mapa_base=raster(nrows=88, ncols=180,xmn=0,xmx=358, ymn=-88,ymx=88) # Dimensiones del raster
  val=c(as.matrix(t(x),ncol=1,byrow = T)) 
  val=as.numeric(val)
  val[val==-999.000]=NA
  values(mapa_base)=val
  return(mapa_base)
}

# Con esta función se depura y rasteriza la tabla .tsv
rasterize=function(dates) { 
  
  if(require(raster)==FALSE){install.packages("raster")}
  library("raster")
  pos_years=!is.na(dates[1,]) # Muestra en que lugares de la fila 1 hay información
  year_month=dates[1,][pos_years] # Muestra la información d ela fila
  year=substr(year_month[-1],1,4) # Substrae el año de las fehcas
  ## Muestra las posiciones de las filas en la tabla que no contienen información relevante para el raster
  total_row_delete=c(-1,-3,-(which(dates[,1]=="88.0")-2),-which(dates[,1]=="-88.0"),-which(dates[,1]==""))
  dates=dates[total_row_delete,-1] # Elimina la información no relevante
  
  list_dates=split(dates,sort(rep(year,88))) # Se divide la tabla de datos por año
  all_raster=lapply(list_dates,transform_raster) ## Transforma las tablas de datos en rasters
  layers=stack(all_raster) # Crea un stack
  layers_crop=crop(layers,extent(0, 358, -30, 30)) ## Realiza el corte para que solo tome el tropico
  return(layers_crop)
}

### Esta función convierte los datos de las estacines en datos trimestrales
data_trim=function(Estaciones_C, a){ #Los argumentos son el conjunto de las estaciones 
  ## y el mes de incio del periodo (a)
  stations=Estaciones_C 
  stations=stations[-1:-2,] # Quite las dos primeras filas (coordenadas)
  year=sort(rep(1981:2013,12)) # cree un vector para los años
  month=rep(1:12,length(1981:2013)) # cree el vector de meses
  data_station=cbind.data.frame(year,month,stations, row.names=NULL) # cree un data frame 
  pos=seq(a,dim(data_station)[1],12) #  posiciones ne las que se encuentra el mes a
  pos_select=sort(c(pos,pos+1,pos+2)) # muestre las posiciones del trimestre
  # Agregue los datos del trimestre y luego sumelos. 
  data_out=aggregate(data_station[pos_select,-1:-2],by=list(sort(rep(1:(length(pos)),3))),sum)
  data_out_final=na.omit(data_out[,-1]) # Elimine los NA
  years_y=na.omit(year[pos+1]) # Elimine los NA
  data_out_final=data.frame(years_y,data_out_final) # Cree un data frame con los datos finales
  return(data_out_final)} # devuelva los datos finales


### Funcion coeficiente de correlacion de Pearson
# Permite calcular el mapa de correlaciones de una estacion en particular
# station = nombre o coluumna de la estacion a la cual se le desea calcular el mapa de correlaciones
# variable oceanoatmosferica (TSM en este caso) con la cual se desea calcular las correlaciones
pearson_1<-function(station, var_ocanoAt){
  #### Variación de las estaciones
  
  #### Correlación de Pearson 
  ocean=which(!is.na(var_ocanoAt[[1]][])) # tome las posiciones en las que la variable sea diferente de NA
  correl=array(NA,length(ocean)) # relice un arreglo del tamaño de oceano 
  var_table=var_ocanoAt[] # Realice una tabla de la variable

  for(i in 1:length(ocean)){ # En todos los pixeles diferentes de NA
    var_pixel=var_table[ocean[i],] # Extraiga el pixel i 
    correl[i]=cor(station,var_pixel, method = "pearson") # realice la correlación entre el pixel i el modo 1 de x
  } # 
  
  correl_map_p=var_ocanoAt[[1]] # Cree un raster vacio 
  correl_map_p[]=NA 
  correl_map_p[ocean]=correl # Almacene en el raster los NA 
  
  return(correl_map_p)}

### Funcion coeficiente de correlacion de Spearman
# Permite calcular el mapa de correlaciones de una estacion en particular
# station = nombre o coluumna de la estacion a la cual se le desea calcular el mapa de correlaciones
# variable oceanoatmosferica (TSM en este caso) con la cual se desea calcular las correlaciones
spearman_1<-function(station, var_ocanoAt){
  #### Correlación de Spearman
  ocean=which(!is.na(var_ocanoAt[[1]][])) # tome las posiciones en las que la variable sea diferente de NA
  correl=array(NA,length(ocean)) # relice un arreglo del tamaño de oceano 
  var_table=var_ocanoAt[] # Realice una tabla de la variable
  
  for(i in 1:length(ocean)){ # En todos los pixeles diferentes de NA
    var_pixel=var_table[ocean[i],] # Extraiga el pixel i 
    correl[i]=cor(station,var_pixel, method = "spearman") # realice la correlación entre el pixel i el modo 1 de x
  } # 
  
  correl_map_s=var_ocanoAt[[1]] # Cree un raster vacio 
  correl_map_s[]=NA 
  correl_map_s[ocean]=correl # Almacene en el raster los NA 

  return(correl_map_s)}

## extract_function, permite extraer solo los valores de un raster mayores a 0.5 en valor absoluto
## Sirve para filtrar las refiones con alta correlación de un mapa de correlaciones
## lista = es un objeto tipo lista de entrada (que contiene varios raster)
## en caso de que lsita sea un stack cortara los raster respecto al primer raster
extract_function<-function(lista){
  correl_1=lista # inicialice un raster
  correl_1[]=NA  # todos sus valores son NA
  correl_1[which(abs(lista[])>0.5)]<-lista[which(abs(lista[])>0.5)] # almacene los valores 
  #en las posiciones que cumplen la condición 
  return(correl_1)}



### Esta función realiza los mapas de correlaciones para el departamento,  
### lead, inicio del trimestre y variable de respuesta y deseada
### dep= departamento en el cual se ubica la estación (a pronosticar)
### lead =  nombre del archivo de la sst (MAM, DEF ...)
### a = mes de incio del trimestre
### answer = tipo de variable de respuesta (mean, pca, o el nombre de la estación a pronsticar)
correlation<-function(dep, lead, a, answer){
  ### Hay que leer el archivo de estaciones y seleccionar una sola (preliminar)
  Estaciones_C <- read.delim(paste(ruta,"/dep/precip_",dep,".txt",sep=""),skip =3, header=T, sep="")
  SST<-read.table(paste(ruta,"/ERSST_CPT/",lead,".tsv",sep=""),sep="\t",dec=".",skip =3,fill=TRUE,na.strings =-999)
  
  ## Conversión a raster de la SST y de los datos de las estaciones a trimestrales
  SST=rasterize(SST)
  data<-data_trim(Estaciones_C, a)
  ## Condición de los archivos dependiendo del trimestre a pronosticar
  if(a == 12){
    data<-data[data$years_y!=1982,]
    var_ocanoAt <-SST[[paste("X", 1983:2013,sep="")]]
  } else  if(a !=12){
    data<-data[data$years_y!=1981,]
    var_ocanoAt <-SST[[paste("X", 1982:2013,sep="")]]
  }
  # elimine la columna de los a;is 
  data=data[,-1]
  # inicialice el vector estación 
  station<-0
  ### Condición para la variable de respuesta
  if(answer=="mean"){
    station =  apply(data, 1, mean) 
  } else if(answer == "pca"){
    pca<-dudi.pca(t(data),scan = FALSE)
    station<-pca$co
  } else station = data[,answer]
  
  #### Correlación de Pearson 
  correl_map_p<-pearson_1(station, var_ocanoAt)
  
  
  ### Correlación de Spearman
  correl_map_s<-spearman_1(station, var_ocanoAt)
  
  
  ### Cree un stack para los mapas de correlaciones
  correl<-stack(correl_map_p, correl_map_s)
  names(correl)=c("Pearson", "Spearman")
  rasters<-list(ind=station, data=data, SST=var_ocanoAt, maps=correl)
return(rasters)}
### Esta función retorna la variable y seleccionada
### los datos de la estación trimestrales (sin la variable year)
### los datos de la sst para el lead seleccionado (depurados de acuerdo al triemstre)
### y los mapas de correlaciones (tanto para pearson como spearman en un stack)



### Esta función entrega los mapas de correlaciones de todas las estaciones de un departamento
### Permite obtener un area general con las regiones más frecuentes
### dep = nombre del archivo de departamentos (casanare)
### lead = nombre del archivo de de la TSM con el que se realizara el ajuste (lead)
### a = mes de incio del trimestre
sta_complete<-function(dep, lead, a){
  
  ### Hay que leer el archivo de estaciones y seleccionar una sola (preliminar)
  Estaciones_C <- read.delim(paste(ruta,"/dep/precip_",dep,".txt",sep=""),skip =3, header=T, sep="")
  SST<-read.table(paste(ruta,"/ERSST_CPT/",lead,".tsv",sep=""),sep="\t",dec=".",skip =3,fill=TRUE,na.strings =-999)
  
  ## Conversión a raster de la SST y de los datos de las estaciones a trimestrales
  SST=rasterize(SST)
  data<-data_trim(Estaciones_C, a)
  ## Condición de los archivos dependiendo del trimestre a pronosticar
  if(a == 12){
    data<-data[data$years_y!=1982,]
    var_ocanoAt <-SST[[paste("X", 1983:2013,sep="")]]
  } else  if(a !=12){
    data<-data[data$years_y!=1981,]
    var_ocanoAt <-SST[[paste("X", 1982:2013,sep="")]]
  }
  # elimine la columna de los a;is 
  data=data[,-1]
  
  ### Correlación de Pearson para todas las estaciones del departamento
  prueba_pearson<-list() # cree una lista donde se almacenan los rasters
  prueba_pearson_1<-apply(data, 2, pearson_1, var_ocanoAt ) # haga un mapa de correlación
  # por cada columna
  prueba_pearson<-stack(prueba_pearson_1) # cree un stack
  
  ### Correlación de Spearman para todas las estaciones del departamento
  prueba_spearman<-list()# cree una lista donde se almacenan los rasters
  prueba_spearman_1<-apply(data, 2, spearman_1, var_ocanoAt)# haga un mapa de correlación
  # por cada columna
  prueba_spearman<-stack(prueba_spearman_1) # cree un stack
  
  ## Cree un mapa de las regiones más frecuentes (areas con correlación mayor a 0.5)
  areas_s<-sum(abs(prueba_spearman)>0.5)
  areas_p<-sum(abs(prueba_pearson)>0.5)
  
  ## Haga los cortes de las zonas con correlaciones altas 
  cut_s<-list()
  cut_s<-lapply(prueba_spearman_1, extract_function)
  cut_s<-stack(cut_s)
  
  cut_p<-list()
  cut_p<-lapply(prueba_pearson_1, extract_function)
  cut_p<-stack(cut_s)
  
  # Almacene las respuestas en una lista
  answer<-list(prueba_pearson, prueba_spearman, areas_p, areas_s,  cut_p, cut_s)
  names(answer)=c("cor_pear", "cor_spe", "areas_p", "areas_s", "cut_p", "cut_s")
return(answer)}


#for(i in 1:10){print(i);print(paste("prueba",i, sep=""))}

#### Declaración paletas de colores para los gráficos

jet.colors <-colorRampPalette(c( "white", "yellow", "#FF7F00", "red", "#7F0000"))
jet=colorRampPalette(c("#2166AC", "snow","#B2182B" ))


myThemec <- BuRdTheme()
myThemec$regions$col=colorRampPalette(c( "white", "yellow", "#FF7F00", "red", "#7F0000", "#B72330", "#B41D2D", "#B2182B"))(30)
myThemec$panel.background$col = "gray30"



myTheme <- BuRdTheme()
myTheme$regions$col=colorRampPalette(c("#2166AC", "white","snow","#B2182B"))(20)
myTheme$panel.background$col = "gray30"


myTheme1 <- BuRdTheme()
myTheme1$regions$col=colorRampPalette(c("#2166AC", "snow","#B2182B"))(20)


myTheme2 <- BuRdTheme()
myTheme2$regions$col=colorRampPalette(c("black"))


map<-raster(paste(ruta,"/map.tif", sep = ""))
plot(map)



###### Corridas preliminares para determinar la región predictora

### Almacene los continentes para realizar los gráficos
#map=results$SST[[1]]
#map[which(is.na(results$SST[[1]][]))][]=0
#map[which(map[]!=0)]=NA
##writeRaster(map,paste(ruta,"/map.tif",sep=""))
estaciones<-c("DoctrinaLa","AptoYopal","AptoPerales","CentAdmoLaUnion","Nataima","Turipana", "StaIsabel")
sitios<-c("Lorica","Yopal","Ibagué","LaUnion","Espinal","Cereté", "Villanueva")
cbind(estaciones, sitios)


dep="santander"
answer="StaIsabel"
lead<-c("MAM",	"Feb", "Nov", "JJA",	"May",	"Feb", "SON", "Aug",	"May", "DEF",	"Nov",	"Aug")
lead_num<-rep(c("sim",0,3), 4)
a <-rep(seq(3,12,3), each = 3)
graphs<- function(results, file){

 complete<-levelplot(results$maps, par.settings=myTheme)

  cortes<-list()
  cortes[[1]]<-extract_function(results$maps$Pearson)
  cortes[[2]]<-extract_function(results$maps$Spearman)
  cortes<-stack(cortes)
  names(cortes)=c("Pearson", "Spearman")
  curts<-levelplot(cortes, par.settings=myTheme1) + levelplot(map, par.settings=myTheme2)
  
  tiff(paste(ruta, "/cor",file,".tif",sep=""), height=650,width=1000,res=100,
       compression="lzw") 
  print(grid.arrange(complete,curts,ncol=1))
  dev.off()
}
for(i in 1:12){
  results<-correlation(dep, lead[i], a[i], answer)
  file<-paste(answer,"_", a[i], "_", lead_num[i], sep="")
  graphs(results, file)
}  















### All stations
estaciones<-c("DoctrinaLa","AptoYopal","AptoPerales","CentAdmoLaUnion","Nataima","Turipana","StaIsabel")
sitios<-c("Lorica","Yopal","Ibagué","LaUnion","Espinal","Cereté", "Villanueva")
cbind(estaciones, sitios)




dep<-"santander"
lead<-c("MAM",	"Feb", "Nov", "JJA",	"May",	"Feb", "SON", "Aug",	"May", "DEF",	"Nov",	"Aug")
lead_num<-rep(c("sim",0,3), 4)
a <-rep(seq(3,12,3), each = 3)


### Para guardar todos los gráficos
for(i in 1:length(a)){
  all_stations<-sta_complete(dep, lead[i], a[i])
  file<-paste(dep,"_", a[i], "_", lead_num[i], sep="")
  tiff(paste(ruta, "/results_graphs/all_stations/", dep,"/corEcp",file,".tif",sep=""), height=650,width=1000,res=100,
       compression="lzw") 
  print(levelplot(all_stations$cor_pear, par.settings=myTheme))
  dev.off()
  
  
  
  tiff(paste(ruta, "/results_graphs/all_stations/", dep,"/corEcs",file,".tif",sep=""), height=650,width=1000,res=100,
       compression="lzw") 
  print(levelplot(all_stations$cor_pear, par.settings=myTheme))
  dev.off()
  
  
  tiff(paste(ruta, "/results_graphs/all_stations/", dep,"/corEcs",file,".tif",sep=""), height=650,width=1000,res=100,
       compression="lzw") 
  print(levelplot(all_stations$cor_spe, par.settings=myTheme))
  dev.off()
  
  
  tiff(paste(ruta, "/results_graphs/all_stations/", dep,"/corEcp_curt",file,".tif",sep=""), height=650,width=1000,res=100,
       compression="lzw") 
  print(levelplot(all_stations$cut_p, par.settings=myTheme1) + levelplot(map, par.settings=myTheme2))
  dev.off()
  
  
  tiff(paste(ruta, "/results_graphs/all_stations/", dep,"/corEcs_curt",file,".tif",sep=""), height=650,width=1000,res=100,
       compression="lzw") 
  print(levelplot(all_stations$cut_s, par.settings=myTheme1) + levelplot(map, par.settings=myTheme2))
  dev.off()
}

dep<-c("casanare","cordoba","tolima","valle", "santander")
for(j in 1:5){
  for(i in 1:length(a)){
    all_stations<-sta_complete(dep[j], lead[i], a[i])
    areas<-stack(all_stations$areas_p, all_stations$areas_s)
    names(areas)=c("Pearson", "Spearman")
    file<-paste(dep[j],"_", a[i], "_", lead_num[i], sep="")
    tiff(paste(ruta, "/results_graphs/all_stations/", dep,"/areaps",file,".tif",sep=""), height=650,width=1000,res=100,
         compression="lzw") 
    print(levelplot(areas, par.settings=myThemec))
    dev.off()
  }
}

  



## Parametros con los que se corre la función 
dep="santander"
lead="DEF" 
a=12
# Solo se corrio la función para los modelos que CPT selecciono como los mejores modelos
all_stations<-sta_complete(dep, lead, a)



# Graficas para seleccionar las regiones predictoras
layout(matrix(c(1,2), ncol=2))
plot(all_stations$areas_p, colNA="gray30", col=jet.colors(20), main="Pearson") 
plot(all_stations$areas_s, colNA="gray30", col=jet.colors(20), main="Spearman") 
graphics.off()


x11()
plot(all_stations$areas_s+all_stations$areas_p, colNA="gray30", col=jet.colors(20), main="Spearman + Pearson") 

### Con estas funciones se puede realizar el area predictora solo dando click encima de la región deseada
e<-drawExtent()
e<-as.numeric(as.character(e))

#region=crop(results$SST, e)
f<-drawExtent(col="blue")
f<-as.numeric(as.character(f))
#region_f<-crop(results$SST, f)
g<-drawExtent(col="chartreuse4")
g<-as.numeric(as.character(g))




h<-drawExtent(col="darkviolet")
h<-as.numeric(as.character(h))



j<-drawExtent(col="pink")


rbind(t(e),t(f),t(g),t(h))

########

## Lea la tabla con las regiones predictoras seleccionadas
tabla=read.table("clipboard",header = T)
#lead_num<-c(3,"sim",3,3,0, 0,0,0,3,0,3, 3,"sim", 0,"sim",3) # Modelo que selecciono CPT
#tabla<-cbind.data.frame(lead_num, tabla)



### Esta función realiza los gráficos de las areas predictoras
### el objeto fila es en el cual se encuentran las coordenadas de las areas predictoras
areas_graph<-function(fila){
## Cree el grafico en ggplot (raster donde solo se gráfica con color los continentes)
  plot_areas<-gplot(all_stations$areas_p) + geom_tile(aes(fill = value)) +
  coord_equal() + theme_bw()+theme(legend.position="none", panel.grid.major = element_line(colour = "white")) + labs(x="Long", y="Lat")+
  scale_fill_gradient2(low="white",mid = "white", high="white",name = " ") 

## Gráfique como rectangulos las areas predictoras sobre el mapa
  plot_areas <- plot_areas + geom_rect(xmin = tabla[fila,5], xmax = tabla[fila,6], ymin = tabla[fila,7], ymax = tabla[fila,8],fill="lightpink", alpha=0.2) #+ geom_text(data = data.frame(), aes(tabla[fila,5]+10, tabla[fila,7]+5, label = "1"))
  
  plot_areas <- plot_areas + geom_rect(xmin = tabla[fila,9], xmax = tabla[fila,10], ymin = tabla[fila,11], ymax = tabla[fila,12],fill="yellow", alpha=0.2) #+ geom_text(data = data.frame(), aes(tabla[fila,9]+10, tabla[fila,11]+5, label = "2"))
  
  plot_areas <- plot_areas + geom_rect(xmin = tabla[fila,13], xmax = tabla[fila,14], ymin = tabla[fila,15], ymax = tabla[fila,16],fill="slateblue", alpha=0.2) #+  geom_text(data = data.frame(), aes(tabla[fila,13]+10, tabla[fila,15]+5, label = "3"))
  
  plot_areas <- plot_areas + geom_rect(xmin = tabla[fila,17], xmax = tabla[fila,18], ymin = tabla[fila,19], ymax = tabla[fila,20],fill="yellowgreen", alpha=0.2) #+geom_text(data = data.frame(), aes(tabla[fila,17]+10, tabla[fila,19]+5, label = "4"))
  
 # plot_areas <- plot_areas + geom_rect(xmin = tabla[fila,21], xmax = tabla[fila,22], ymin = tabla[fila,23], ymax = tabla[fila,24],fill="darkturquoise", alpha=0.2) #+ geom_text(data = data.frame(), aes(tabla[fila,21]+10, tabla[fila,23]+5, label = "5"))
  
  #print(plot_areas)
  ## Guarde las areas predictoras
  
  setwd("C:/Users/AESQUIVEL/Google Drive/salidas_afm/maps_correlation/")
  ggsave(paste(tabla[fila,3],"_",tabla[fila,2],"_",tabla[fila,1],".png", sep = ""),width =6 ,height =1.5,dpi=200 )
}
fila=1:dim(tabla)[1] ## Cree vector de filas
for(i in 1:dim(tabla)[1]){areas_graph(i)} # Gráfique las filas

setwd("C:/Users/AESQUIVEL/Google Drive/salidas_afm/")
getwd()

## Para generar la legenda
#df <- data.frame( x = 1,  y = rep(1:5), z = factor(rep(1:5)))
#gplot(df, aes(x,y, fill=z)) +   geom_tile() + theme_classic() + theme(legend.position="none", panel.background = element_blank())+
#  scale_fill_manual(values= c("lightpink","yellow", "slateblue","yellowgreen","darkturquoise"))+geom_text(data = df, aes(1, 1:5, label = 1:5))
  















####################################################################################

## Lea la tabla con las coordenadas de las regiones
tabla=read.table("clipboard",header = T)
# Agregue el lead con el que se obtuvieron a la tabla 
#lead_num<-c(3,"sim",3,3,0, 0,0,0,3,0,3, 3,"sim", 0,"sim",3)
#tabla<-cbind.data.frame(lead_num, tabla)

# Esta información se usa como referencia, son los sitios 
# de interes en el estudio
estaciones<-c("DoctrinaLa","AptoYopal","AptoPerales","CentAdmoLaUnion","Nataima","Turipana", "StaIsabel")
sitios<-c("Lorica","Yopal","Ibagué","LaUnion","Espinal","Cereté", "Villanueva")
cbind(estaciones, sitios)


## Esta función permite realizar el AFM
## Esta configurada para una tabla con 20 columnas de 
## las regiones se encuentran entre las columnas 4 y 19
## Maximo se pueden ingresar 5 regiones predictoras en la función
## dep = departamento 
## a = inicio del trimestre
## lead = nombre del archivo lead time
## results = resultados de la función correlation 
## answer = estación variable de respuesta 
MFA_C<-function(dep, a, lead,lead_num, num_zone, results, answer){
  # determine el trimestre que se va a pronosticar en base al mes de incio del periodo 
  if(a==3){trim<-"MAM"}else if(a==6){ trim<-"JJA"}else if(a==9){trim<-"SON"}else if(a==12){trim<-"DEF"}else print("ERRORR !!!!")
  
  ## nombre de la fila que se va a estudiar (departamento - trimestre donde se encuentra las estaciones)
  fila=paste(answer,trim,sep="_")
  regiones<-tabla[fila,names(tabla)[5:20]] # elimine las celdas que no corresponden a coordenadas
  regiones<-regiones[1:(4*num_zone)] #determine cuantas regiones se utilizaran
  
  ### AFM condicionado por el número de regiones
  if(num_zone==2){ # 2 zonas 
    # Cree la región predictora y corte la SST de acuerdo a la región
    region_1<-extent(as.numeric(regiones[1:4])); region_1=crop(results$SST, region_1)
    region_2<-extent(as.numeric(regiones[5:8])); region_2=crop(results$SST, region_2)
    
    # cree un data frame de las regiones predictoras omitiendo las celdas sin información
    zonas<-as.data.frame(rbind(na.omit(region_1[]), na.omit(region_2[])))
    # Corra el AFM, tenga encuenta que el argumento type="s" ("s" variables are scaled to unit variance)
    # En esta linea se tiene que delimitar claramente los grupos
    res <- MFA(t(zonas), group=c(dim(na.omit(region_1[]))[1],dim(na.omit(region_2[]))[1]),type=rep("s",2), name.group=c("zona1", "zona2"))

    
  } else if(num_zone==3){## 3 zonas
    
    region_1<-extent(as.numeric(regiones[1:4])); region_1=crop(results$SST, region_1)
    region_2<-extent(as.numeric(regiones[5:8])); region_2=crop(results$SST, region_2)
    region_3<-extent(as.numeric(regiones[9:12])); region_3=crop(results$SST, region_3)
    
    zonas<-as.data.frame(rbind(na.omit(region_1[]), na.omit(region_2[]), na.omit(region_3[])))
    res <- MFA(t(zonas), group=c(dim(na.omit(region_1[]))[1],dim(na.omit(region_2[]))[1], dim(na.omit(region_3[]))[1]),type=rep("s",3), name.group=c("zona1", "zona2",  "zona3"))
   
    
    
  } else  if(num_zone==4){ ## 4 zonas
    region_1<-extent(as.numeric(regiones[1:4])); region_1=crop(results$SST, region_1)
    region_2<-extent(as.numeric(regiones[5:8])); region_2=crop(results$SST, region_2)
    region_3<-extent(as.numeric(regiones[9:12])); region_3=crop(results$SST, region_3)
    region_4<-extent(as.numeric(regiones[13:16])); region_4=crop(results$SST, region_4)
    
    zonas<-as.data.frame(rbind(na.omit(region_1[]), na.omit(region_2[]), na.omit(region_3[]), na.omit(region_4[])))
    res <- MFA(t(zonas), group=c(dim(na.omit(region_1[]))[1],dim(na.omit(region_2[]))[1], dim(na.omit(region_3[]))[1], dim(na.omit(region_4[]))[1]),type=rep("s",4), name.group=c("zona1", "zona2",  "zona3",  "zona4"))
    
  
  } 
  
  
  setwd(paste("C:/Users/AESQUIVEL/Google Drive/salidas_afm", "/results_graphs/AFM/", dep, "/",answer, "/", trim, "/", sep=""))
  
  
  
   #Guarde las guarde el archivo de contribuciones 
  write.csv(res$group$contrib, file=paste("contr",answer, trim,lead_num ,".csv", sep="_"))
   #Guarde el archivo con lso valores propios
  write.csv(res$eig, file=paste("eigGroup",answer, trim,lead_num ,".csv", sep="_"))
  # Imprima el resumen del AFM
  summary(res)
  
  
  ###### Guardado automatico de la grafica de valores propios
  png(file =paste("barEig",answer, trim, lead_num ,".png", sep="_"), bg = "transparent", width = 500, height = 300)
  barplot(res$eig[,1],main=paste("Eigenvalues",dep,trim,"-",lead),names.arg=1:nrow(res$eig))
  dev.off()
  
 
  # correlación entre las componentes y el indicador
  corr<-cbind.data.frame(cor1=cor(results$ind, res$global.pca$ind$coord[,1]),cor2=cor(results$ind, res$global.pca$ind$coord[,2]), cor3=cor(results$ind, res$global.pca$ind$coord[,3]))
  
  # Realice un gráfico entre la primera componente y el indicador de la estación y agregue la correlación
  datos=as.data.frame(cbind(res$global.pca$ind$coord[,1], results$ind))
  gp = ggplot(datos, aes(datos[,1], datos[,2])) +  geom_point(shape=19, size=3,colour="#CC0000")
  gp = gp +  geom_smooth(method=lm, colour="#000099") + labs(x="Primera Componente AFM",y="Precipitación (mm)") 
  gp = gp + geom_text(data = data.frame(), aes(max(datos[,1])-0.5, max(datos[,2])+20, label = paste("r = ", round(cor(datos[,1], datos[,2]),3), sep="")))
  gp = gp + theme_bw()
  # Realice un gráfico entre la segunda componente y el indicador de la estación y agregue la correlación
  datos2=as.data.frame(cbind(res$global.pca$ind$coord[,2], results$ind))
  p2 = ggplot(datos2, aes(datos2[,1], datos2[,2])) +  geom_point(shape=19, size=3,colour="#CC0000")
  p2 = p2 +  geom_smooth(method=lm, colour="#000099") + labs(x="Segunda Componente AFM",y="Precipitación (mm)") 
  p2 = p2 + geom_text(data = data.frame(), aes(max(datos2[,1])-0.5, max(datos2[,2])+20, label = paste("r = ", round(cor(datos2[,1], datos2[,2]),3), sep="")))
  p2 = p2 + theme_bw()
  
  # Guarde las dos imagenes automaticamente
  tiff(paste(ruta,"/results_graphs/AFM/", dep, "/", answer,"/", trim,"/disp","_",answer, "_",trim,"_",lead_num,".tif",sep=""), height=400,width=550,res=100,
       compression="lzw") 
  grid.arrange(gp,p2,ncol=2)
  dev.off()
  # Guarde las componentes en un archivo .csv
  write.csv(res$global.pca$ind$coord, file=paste("componentes",answer, a, lead_num ,".csv", sep="_"))
  # Guarde lo que vaya a entregar en una lista que entrega, la correlaciónm, la contribución y las componentes
  entrega<-list(corr=corr, contrib=res$group$contrib, res_global.pca=res$global.pca$ind$coord[,1:3])
  return(entrega)}
#rownames(tabla)<-paste(tabla[,"Station"],tabla[,1], sep="_")

tabla=read.table("clipboard",header = T)
rownames(tabla)<-paste(tabla[,"Station"],tabla[,1], sep="_")



dep="santander"
## lead
if(a==12){
  lead=c("DEF","Nov", "Aug")
}else  if(a==3){ 
  lead=c("MAM","Feb", "Nov")}else  if(a==6){
    lead=c("JJA","May","Feb")
  }else  if(a==9){
    lead=c("SON", "Aug", "May")
  }


answer= "StaIsabel"
lead_num=c("sim",0,3)
a<-c(12,3,6,9)
num_zone<-tabla[,"num_reg"]

for(j in 1:4){
  if(a[j]==12){
    lead=c("DEF","Nov", "Aug")
  }else if(a[j]==3){ 
    lead=c("MAM","Feb", "Nov")}else if(a[j]==6){
      lead=c("JJA","May","Feb")
    }else if(a[j]==9){
      lead=c("SON", "Aug", "May")}
  for(i in 1:3){
    results<-correlation(dep, lead[i], a[j],answer)
    MFA<-MFA_C(dep, a[j], lead[i], lead_num[i], num_zone[j], results, answer)
  }
} 


setwd("C:/Users/AESQUIVEL/Google Drive/salidas_afm/")
getwd()







## Esta función es similar a la anterior, con la diferencia que no guarda auntomaticamente
## los archivos
MFA_P<-function(dep, a, lead, num_zone, results, answer){
  if(a==3){trim<-"MAM"}else if(a==6){ trim<-"JJA"}else if(a==9){trim<-"SON"}else if(a==12){trim<-"DEF"}else print("ERRORR !!!!")
  fila=paste(answer,trim,sep="_")
  regiones<-tabla[fila,names(tabla)[5:20]]
  regiones<-regiones[1:(4*num_zone)]
  ### Regiones
  if(num_zone==2){ # 2 zonas
    region_1<-extent(as.numeric(regiones[1:4])); region_1=crop(results$SST, region_1)
    region_2<-extent(as.numeric(regiones[5:8])); region_2=crop(results$SST, region_2)
    
    zonas<-as.data.frame(rbind(na.omit(region_1[]), na.omit(region_2[])))
    res <- MFA(t(zonas), group=c(dim(na.omit(region_1[]))[1],dim(na.omit(region_2[]))[1]),type=rep("s",2), name.group=c("zona1", "zona2"))
    
    
  } else  if(num_zone==3){## 3 zonas
    
    region_1<-extent(as.numeric(regiones[1:4])); region_1=crop(results$SST, region_1)
    region_2<-extent(as.numeric(regiones[5:8])); region_2=crop(results$SST, region_2)
    region_3<-extent(as.numeric(regiones[9:12])); region_3=crop(results$SST, region_3)
    
    zonas<-as.data.frame(rbind(na.omit(region_1[]), na.omit(region_2[]), na.omit(region_3[])))
    res <- MFA(t(zonas), group=c(dim(na.omit(region_1[]))[1],dim(na.omit(region_2[]))[1], dim(na.omit(region_3[]))[1]),type=rep("s",3), name.group=c("zona1", "zona2",  "zona3"))

    
  } else  if(num_zone==4){ ## 4 zonas
    region_1<-extent(as.numeric(regiones[1:4])); region_1=crop(results$SST, region_1)
    region_2<-extent(as.numeric(regiones[5:8])); region_2=crop(results$SST, region_2)
    region_3<-extent(as.numeric(regiones[9:12])); region_3=crop(results$SST, region_3)
    region_4<-extent(as.numeric(regiones[13:16])); region_4=crop(results$SST, region_4)
    
    zonas<-as.data.frame(rbind(na.omit(region_1[]), na.omit(region_2[]), na.omit(region_3[]), na.omit(region_4[])))
    res <- MFA(t(zonas), group=c(dim(na.omit(region_1[]))[1],dim(na.omit(region_2[]))[1], dim(na.omit(region_3[]))[1], dim(na.omit(region_4[]))[1]),type=rep("s",4), name.group=c("zona1", "zona2",  "zona3",  "zona4"))
    
  } else  if(num_zone==5){
    region_1<-extent(as.numeric(regiones[1:4])); region_1=crop(results$SST, region_1)
    region_2<-extent(as.numeric(regiones[5:8])); region_2=crop(results$SST, region_2)
    region_3<-extent(as.numeric(regiones[9:12])); region_3=crop(results$SST, region_3)
    region_4<-extent(as.numeric(regiones[13:16])); region_4=crop(results$SST, region_4)
    region_5<-extent(as.numeric(regiones[17:20])); region_5=crop(results$SST, region_5)
    
    zonas=as.data.frame(rbind(na.omit(region_1[]), na.omit(region_2[]), na.omit(region_3[]), na.omit(region_4[]), na.omit(region_5[])))
    res <- MFA(t(zonas), group=c(dim(na.omit(region_1[]))[1],dim(na.omit(region_2[]))[1], dim(na.omit(region_3[]))[1], dim(na.omit(region_4[]))[1], dim(na.omit(region_5[]))[1]),type=rep("s",5), name.group=c("zona1", "zona2",  "zona3",  "zona4",  "zona5"))

  }
  corr<-cbind.data.frame(cor1=cor(results$ind, res$global.pca$ind$coord[,1]),cor2=cor(results$ind, res$global.pca$ind$coord[,2]), cor3=cor(results$ind, res$global.pca$ind$coord[,3]))
  corr_p<-cbind.data.frame(cor1t=cor.test(results$ind, res$global.pca$ind$coord[,1])$p.value,cor2t=cor.test(results$ind, res$global.pca$ind$coord[,2])$p.value,cor3t=cor.test(results$ind, res$global.pca$ind$coord[,3])$p.value)
  
  entrega<-list(corr=corr, corr_test=corr_p, contrib=res$group$contrib, res_global.pca=res$global.pca$ind$coord[,1:3])
  return(entrega)}



tabla=read.table("clipboard",header = T)
rownames(tabla)<-paste(tabla[,"Station"],tabla[,1], sep="_")

answer<- "StaIsabel"
dep<-"santander"
a<-tabla[,"a"]
num_zone<-tabla[,"num_reg"]

datos<-0
for(j in 1:4){
  
   if(a[j]==12){
     lead=c("DEF","Nov", "Aug")
   }else if(a[j]==3){ 
     lead=c("MAM","Feb", "Nov")}else if(a[j]==6){
       lead=c("JJA","May","Feb")
     }else if(a[j]==9){
       lead=c("SON", "Aug", "May")}
  
  for(i in 1:3){
    results<-correlation(dep, lead[i], a[j],answer)
    MFA<-MFA_P(dep, a[j], lead[i], num_zone[j], results, answer)
    datos_p=data.frame(a[j],answer,lead[i], MFA$corr, MFA$corr_test)
    datos=rbind(datos, datos_p)
  }
} # Aqui se almacena el coeficiente de correlación de las 3 primeras componentes
# Cree una tabla global para el coeficiente de correlación 
datosp<-rbind(datosp,datos[-1,])


##Organice la información global 
#datosp<-datosp[-1,]
names(datosp)<-c("a","Station","lead","cor1", "cor2", "cor3", "test1", "test2","test3")
row.names(datosp)<-paste(substring(datosp[,2],1,5),datosp[,1], datosp[, "lead"], sep="_")
## Guarde en un archivo la tabla
write.csv(datosp, file = "correlaciones.csv")




#### Gráfico de Correlaciones
correlaciones <- read.csv("C:/Users/AESQUIVEL/Google Drive/salidas_afm/correlaciones.csv", row.names=1)
correlaciones[,"a"]<-as.factor(correlaciones[,"a"])


# Este es un gráfico de barras de las correlaciones de cada componente 
# con cada estación agrupadas por trimestre y lead time 
c <- ggplot(correlaciones, aes(a,cor1, fill=as.factor(lead_num)))
c <- c + geom_bar(stat = "identity", position="dodge")
c <- c + facet_grid(. ~ Station) + theme_bw() 
c <- c + geom_hline(yintercept = c(0.25,-0.25)) + labs( x="", y="Cor comp 1") + ylim(c(-0.8,0.8)) + guides(fill=guide_legend(title="Lead Time"))


d <- ggplot(correlaciones, aes(a,cor2, fill=as.factor(lead_num)) )
d <-d + geom_bar(stat = "identity", position="dodge")
d <- d + facet_grid(. ~ Station) + theme_bw() 
d <- d + geom_hline(yintercept = c(0.25,-0.25)) + labs( x="", y="Cor comp 2") + ylim(c(-0.8,0.8)) + guides(fill=guide_legend(title="Lead Time"))

f <- ggplot(correlaciones, aes(a,cor3, fill=as.factor(lead_num)) )
f <-f + geom_bar(stat = "identity", position="dodge")
f <- f + facet_grid(. ~ Station) + theme_bw() 
f <-f + geom_hline(yintercept = c(0.25,-0.25)) + labs( x="", y="Cor comp 3") + ylim(c(-0.8,0.8)) + guides(fill=guide_legend(title="Lead Time"))

x11()
grid.arrange(c,d,f, ncol=1)



# Posiciones en las que se cumple que el valor absoluto de la correlación
# sea mayor a 0.25
cond<-ifelse(abs(correlaciones[,4:6])>0.25,yes = 1,no = 0)

suma<-apply(cond, 1, sum) # Diga por filas cuantas componentes cumplen la condición 

rownames(correlaciones)[which(suma<1)] # Muestre cuales modelos no pueden ajustarse
# es decir no  cumplen con la condición 
correlaciones[which(suma<1),] # Muestre las correlaciones de aquellos modelos que no cumplen























### 49 modelos seran evaluados en esta sección los que tengan valor p < 0.05 no 
### Es decir con un nivel de confianza de 0.95
### Pasaran a ser evaluados en la siguiente fase 


# Lea la tabla donde se tienen las coordenadas de las regiones predictoras
tabla=read.table("clipboard",header = T)
rownames(tabla)<-paste(tabla[,"Station"],tabla[,"Trimestre" ], sep="_")

# Lea la tabla en la cual tiene almacenados los modelos que se correran 
# En esta tabla se tiene la información de cuales componentes ingresaran al modelo
tab_comp<-read.table("clipboard",header = T)
rownames(tab_comp)<-paste(substring(tab_comp[,2],1,5),tab_comp[,1], tab_comp[,3], sep="_")


## Esta función realiza el modelo de regresión lineal multiple (o simple)
## Se seleccionan cuantas componentes entran a la modelación y automaticamente guarda información 
## relevante sobre el modelo como valores observados, pronosticados, gráfico de supuestos
## Tambien entrega en una lista indicadores como RMSE, R^2-adj, entre otros
## dep = departamento donde se encuentra la estación 
## lead = lead time con el que se pronostica ("MAM", "Feb")
## lead_num = lead time con el que se pronostica ("Sim", "0", "3")
## a = mes de incio del trimestre
## answer = estación variable de respuesta (ej = "Turipana", "Nataima", ...)
## num_zone = número de zonas con el que se realiza el AFM
## comp = vector de tamaño 3 de 0 y 1 (1 indica que esa componente va a ser utilizada en el modelo)
modelo<-function(dep,lead,lead_num, a, answer, num_zone, comp){
  
  # Con esta función se consiguen los valores de la SST y de la estación (answer) deseada 
  # (además de mapas de correlaciones e información departamental)
  results<-correlation(dep, lead, a,answer)
  # Esta función corre el AFM y entrega las componentes para ingresar al modelo
  MFA<-MFA_P(dep, a, lead, num_zone, results, answer)
  
  # En este condicional se ajusta el modelo de regresión lineal
  # dependiendo de los valores del vector cop se decide que tipo de modelo se ajsuta
  # es decir con cuantas y cuales componentes
  # demolo de la forma lm(results$ind~MFA$componentes)
  if(comp[1]==1 & comp[2]==1 & comp[3]==1){
    modelo<-lm(results$ind~MFA$res_global.pca[,1]+MFA$res_global.pca[,2]+MFA$res_global.pca[,3])
  }else if(comp[1]==1 & comp[2]==1 & comp[3]==0){
    modelo<-lm(results$ind~MFA$res_global.pca[,1]+MFA$res_global.pca[,2])
  }else if(comp[1]==1 & comp[2]==0 & comp[3]==1){
    modelo<-lm(results$ind~MFA$res_global.pca[,1]+MFA$res_global.pca[,3])
  }else if(comp[1]==1 & comp[2]==0 & comp[3]==0){
    modelo<-lm(results$ind~MFA$res_global.pca[,1])
  }else if(comp[1]==0 & comp[2]==1 & comp[3]==1){
    modelo<-lm(results$ind~MFA$res_global.pca[,2]+MFA$res_global.pca[,3])
  }else if(comp[1]==0 & comp[2]==1 & comp[3]==0){
    modelo<-lm(results$ind~MFA$res_global.pca[,2])
  }else if(comp[1]==0 & comp[2]==0 & comp[3]==1){
    modelo<-lm(results$ind~MFA$res_global.pca[,3])
  }else if(comp[1]==0 & comp[2]==0 & comp[3]==0){
    print(modelo<-"ERRORRR !!!")
  }
  
  
  ### Cambia el directorio de guardado de los archivos
  setwd(paste("C:/Users/AESQUIVEL/Google Drive/salidas_afm", "/results_graphs/models/", dep, "/",answer, "/", sep=""))
  
  
  adj_r_squared=summary(modelo)$adj.r.squared # Estrae el adj_r_squared del modelo
  # A  partir del estadístico F se calcula el valor_p del modelo
  p_value<-1-pf(summary(modelo)$fstatistic[1], summary(modelo)$fstatistic[2], summary(modelo)$fstatistic[3])
  summary(modelo) # imprima el resumen del modelo
  
  # Guarde el resumen gráfico de los supuestos
  tiff(paste(ruta,"/results_graphs/models/",dep,"/", answer,"/Csup_", answer,"_",a,"_",lead_num,".tif",sep=""), height=450,width=700,res=100,
       compression="lzw") 
  layout(matrix(1:6,2)) # ponga todos los gráficos en el display (dividido)
  plot(modelo) # Realiza el resumen gráfico del modelo
  # Correlogramas para verificar el supuesto de independencia (temporal)
  acf(modelo$residuals)
  pacf(modelo$residuals) 
  dev.off()
  
  # Guarde otros indicadores relevantes tales como la media, el intercepto, y las pruebas sobre los supestos
  # tambien el RMSE y la correlación de pearson entre los obs y lo pronsticado
  media<-mean(results$ind) 
  Intercept<-modelo$coefficients[1]
  var<-bptest(modelo)$p.value # igualdad de varianza
  nor<-shapiro.test(modelo$residuals)$p.value # normalidad
  box<-Box.test (modelo$residuals, lag = 5)$p.value # independencia
  cor_b<-round(cor(results$ind, modelo$fitted.values),3)
  RMSE_b<-round(sqrt(sum((modelo$fitted.values-results$ind)^2)/length(results$ind)),3)
  
  # Cree un data frame con todos los indicadores y sumele el nú
  summary_m<-data.frame(media,Intercept,adj_r_squared,p_value, var, nor, box, RMSE_b, num_coef=length(modelo$coefficients), row.names = NULL)
  
  # Guarde automaticamente en un archivo .csv lo obervado, pronosticado y los residuales
  results_model<-cbind.data.frame(obs=results$ind, pron=modelo$fitted.values, residuales=modelo$residuals)
  write.csv(results_model, paste("CresultsModel_", answer,"_",a,"_",lead_num,".csv",sep=""))
  
  # Guarde automaticamente un gráfico de dispersión entre lo observado y lo pronosticado
  ajuste=ggplot(data.frame(), aes(results$ind, modelo$fitted.values)) +  geom_point(shape=19, size=3,colour="#CC0000")
  ajuste=  ajuste +  geom_smooth(method=lm, colour="#000099") + labs(x="Precipitación obs (mm)",y="Precipitación pron (mm)") + theme_bw()
  ajuste=  ajuste +  geom_text(data = data.frame(), aes(max(results$ind)-70, min(modelo$fitted.values)+60, label = paste("r = ", round(cor(results$ind, modelo$fitted.values),3), sep="")))
  ajuste=  ajuste +  geom_text(data = data.frame(), aes(max(results$ind)-70, min(modelo$fitted.values)+10, label = paste("RMSE = ", round(sqrt(sum((modelo$fitted.values-results$ind)^2)/length(results$ind)),3), sep="")))
  
  tiff(paste(ruta,"/results_graphs/models/",dep,"/", answer,"/Cajuste_", answer,"_",a,"_",lead_num,".tif",sep=""), height=450,width=700,res=100,
       compression="lzw") 
  print(ajuste)
  dev.off()
  
  # Cree una lista donde se guarde los indicadores del modelo, lo observado, lo pronosticado y los residuales del modelo, tambien se almacenan las coefficientes
  entrega<-list(summary_m=summary_m, obs=results$ind, pron=modelo$fitted.values, residuales=modelo$residuals,  coeff=modelo$coefficients)
  
# Devuelva la lista 
return(entrega)}

# Corra todos los modelos
dep<-"santander"
answer<-"StaIsabel"
lead<-tab_comp[,"lead"]
lead_num<-tab_comp[,"lead_num"]
a<-tab_comp[,"a"]
num_zone<-tab_comp[,"num_reg"]
comp<-tab_comp[,c("cor1","cor2", "cor3")]

mo_p<-0

# Guarde para todos los modelos los indicadores calculados en un data frame
for(i in 1:dim(tab_comp)[1]){
  model<-modelo(dep,lead[i],lead_num[i],a[i], answer, num_zone[i], comp[i,])$summary_m
  row.names(model)=paste(substring(answer,1,5),a[i], lead_num[i], sep="_")
  mo_p=rbind(mo_p, model)  
}

# Guarde automaticamente una tabla con todos los indicadores de los modelos corridos
mop<-rbind(mop,mo_p[-1, ])
mop

setwd("C:/Users/AESQUIVEL/Google Drive/salidas_afm/")
getwd()
write.csv(mop, file = "summary_models_santander.csv", row.names = TRUE)












### Modelo parcial 

## Esta función es similar a la anterior, con la diferencia que no guarda auntomaticamente
## los archivos
modelop<-function(dep,lead,lead_num, a, answer, num_zone, comp){
  results<-correlation(dep, lead, a,answer)
  MFA<-MFA_P(dep, a, lead, num_zone, results, answer)
  
  if(comp[1]==1 & comp[2]==1 & comp[3]==1){
    modelo<-lm(results$ind~MFA$res_global.pca[,1]+MFA$res_global.pca[,2]+MFA$res_global.pca[,3])
  }else if(comp[1]==1 & comp[2]==1 & comp[3]==0){
    modelo<-lm(results$ind~MFA$res_global.pca[,1]+MFA$res_global.pca[,2])
  }else if(comp[1]==1 & comp[2]==0 & comp[3]==1){
    modelo<-lm(results$ind~MFA$res_global.pca[,1]+MFA$res_global.pca[,3])
  }else if(comp[1]==1 & comp[2]==0 & comp[3]==0){
    modelo<-lm(results$ind~MFA$res_global.pca[,1])
  }else if(comp[1]==0 & comp[2]==1 & comp[3]==1){
    modelo<-lm(results$ind~MFA$res_global.pca[,2]+MFA$res_global.pca[,3])
  }else if(comp[1]==0 & comp[2]==1 & comp[3]==0){
    modelo<-lm(results$ind~MFA$res_global.pca[,2])
  }else if(comp[1]==0 & comp[2]==0 & comp[3]==1){
    modelo<-lm(results$ind~MFA$res_global.pca[,3])
  }else if(comp[1]==0 & comp[2]==0 & comp[3]==0){
    print(modelo<-"ERRORRR !!!")
  }
  
  adj_r_squared=summary(modelo)$adj.r.squared
  p_value<-1-pf(summary(modelo)$fstatistic[1], summary(modelo)$fstatistic[2], summary(modelo)$fstatistic[3])
  summary(modelo)
  
  tiff(paste(ruta,"/results_graphs/models/seleccion/psup_", answer,"_",a,"_",lead_num,".tif",sep=""), height=450,width=700,res=100,
       compression="lzw") 
  layout(matrix(1:6,2))
  plot(modelo)
  acf(modelo$residuals)
  pacf(modelo$residuals) 
  dev.off()
  
  
  media<-mean(results$ind)
  Intercept<-modelo$coefficients[1]
  var<-bptest(modelo)$p.value # igualdad de varianza
  nor<-shapiro.test(modelo$residuals)$p.value # normalidad
  box<-Box.test (modelo$residuals, lag = 5)$p.value # independencia
  cor_b<-round(cor(results$ind, modelo$fitted.values),3)
  RMSE_b<-round(sqrt(sum((modelo$fitted.values-results$ind)^2)/length(results$ind)),3)
  
  summary_m<-data.frame(media,Intercept,adj_r_squared,p_value, var, nor, box, RMSE_b, num_coef=length(modelo$coefficients), row.names = NULL)
  
  
  entrega<-list(summary_m=summary_m, obs=results$ind, pron=modelo$fitted.values, residuales=modelo$residuals,  coeff=modelo$coefficients)
  results_model<-cbind.data.frame(obs=results$ind, pron=modelo$fitted.values, residuales=modelo$residuals)
  
  
  setwd("C:/Users/AESQUIVEL/Google Drive/salidas_afm/results_graphs/models/seleccion/")
  write.csv(results_model, paste("presultsModel_", answer,"_",a,"_",lead_num,".csv",sep=""))
  
  ajuste=ggplot(data.frame(), aes(results$ind, modelo$fitted.values)) +  geom_point(shape=19, size=3,colour="#CC0000")
  ajuste=  ajuste +  geom_smooth(method=lm, colour="#000099") + labs(x="Precipitación obs (mm)",y="Precipitación pron (mm)") + theme_bw()
  ajuste=  ajuste +  geom_text(data = data.frame(), aes(max(results$ind)-70, min(modelo$fitted.values)+60, label = paste("r = ", round(cor(results$ind, modelo$fitted.values),3), sep="")))
  ajuste=  ajuste +  geom_text(data = data.frame(), aes(max(results$ind)-70, min(modelo$fitted.values)+10, label = paste("RMSE = ", round(sqrt(sum((modelo$fitted.values-results$ind)^2)/length(results$ind)),3), sep="")))
  
  tiff(paste(ruta,"/results_graphs/models/seleccion/pajuste_", answer,"_",a,"_",lead_num,".tif",sep=""), height=450,width=700,res=100,
       compression="lzw") 
  print(ajuste)
  dev.off()
  
  return(entrega)}

tab_comp<-read.table("clipboard",header = T)
rownames(tab_comp)<-paste(substring(tab_comp[,2],1,5),tab_comp[,1], tab_comp[,3], sep="_")

dep<-"santander"
answer<-"StaIsabel"
lead<-tab_comp[,"lead"]
lead_num<-tab_comp[,"lead_num"]
a<-tab_comp[,"a"]
num_zone<-tab_comp[,"num_reg"]
comp<-tab_comp[,c("cor1","cor2", "cor3")]

mo_p<-0
for(i in 1:dim(tab_comp)[1]){
  model<-modelop(dep,lead[i],lead_num[i],a[i], answer, num_zone[i], comp[i,])$summary_m
  row.names(model)=paste(substring(answer,1,5),a[i], lead_num[i], sep="_")
  mo_p=rbind(mo_p, model)  
}
mop<-rbind(mop,mo_p[-1, ])

### Guarde automaticamente el archivo con el resumen de los modelos. 
setwd("C:/Users/AESQUIVEL/Google Drive/salidas_afm")
getwd()

write.csv(mop, file = "summary_models_parcial.csv", row.names = TRUE)






















#################### Validación Cruzada

## Esta función realiza la validación cruzada del modelo
## quita periodos de tiempo de 5 años, despues ajusta el modelo
## realiza el pronostico de esos 5 años y los almancena.
## por último realiza la correlación de kendall entre el conjunto de las series prontosicadas
## y la observada. 

## dep = departamento donde se encuentra la estación 
## lead = lead time con el que se pronostica ("MAM", "Feb")
## lead_num = lead time con el que se pronostica ("Sim", "0", "3")
## a = mes de incio del trimestre
## answer = estación variable de respuesta (ej = "Turipana", "Nataima", ...)
## num_zone = número de zonas con el que se realiza el AFM
## comp = vector de tamaño 3 de 0 y 1 (1 indica que esa componente va a ser utilizada en el modelo)
cross_validation <-function(dep,lead,lead_num, a, answer, num_zone, comp){
  
  results<-correlation(dep, lead, a,answer) # corre lo mapas de correlaciones y devuelve
  # la información de la estación y la sst (trimestral)
  MFA<-MFA_P(dep, a, lead, num_zone, results, answer) ## Corre la información del AFM
  
  # Crea una matriz con las posiciones que se deben eliminar en cada validación. 
  j=matrix(c(1:30),5,6, byrow = F) ## Matriz de posiciones
  forecasts=matrix(c(1:30),5,6, byrow = F) ## Matriz en blanco de pronostico
  
  # Crea un data frame con los resultados y las componentes 
  z<-cbind.data.frame(y=results$ind, x1=MFA$res_global.pca[,1], x2=MFA$res_global.pca[,2], x3=MFA$res_global.pca[,3])
  
  ## Crea una matriz para almacenar los coeficientes de regresión
  coef<-matrix(c(1:(sum(comp)+1)),6,(sum(comp)+1), byrow = T)
  
  
  
  # Realiza la validación cruzada 
  for(i in 1:6){
    data<-z[-j[,i],] # elimina las posiciones de z, que almacenadas en la matriz j.
    # condicionando por las diferentes combinaciones de parametros del modelo
    # corre el modelo
    # almacena los pronosticos de las posiciones quitadas
    # alamacena los coeficientes del modelo en cada caso 
    if(comp[1]==1 & comp[2]==1 & comp[3]==1){ 
      modelo<-lm(data$y ~ data$x1 + data$x2 + data$x3)
      forecasts[,i]=modelo$coefficients[1]+modelo$coefficients[2]*z[j[,i],2]+modelo$coefficients[3]*z[j[,i],3]+modelo$coefficients[4]*z[j[,i],4]
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==1 & comp[2]==1 & comp[3]==0){
      modelo<-lm(data$y ~data$x1 + data$x2)
      forecasts[,i]=modelo$coefficients[1]+modelo$coefficients[2]*z[j[,i],2]+modelo$coefficients[3]*z[j[,i],3]
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==1 & comp[2]==0 & comp[3]==1){
      modelo<-lm( data$y ~data$x1 + data$x3)
      forecasts[,i]=modelo$coefficients[1]+modelo$coefficients[2]*z[j[,i],2]+modelo$coefficients[3]*z[j[,i],3]
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==1 & comp[2]==0 & comp[3]==0){
      modelo<-lm(data$y ~data$x1)
      forecasts[,i]=modelo$coefficients[1]+modelo$coefficients[2]*z[j[,i],2]
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==0 & comp[2]==1 & comp[3]==1){
      modelo<-lm(data$y ~ data$x2 + data$x3)
      forecasts[,i]=modelo$coefficients[1]+modelo$coefficients[2]*z[j[,i],2]+modelo$coefficients[3]*z[j[,i],3]
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==0 & comp[2]==1 & comp[3]==0){
      modelo<-lm(data$y ~ data$x2)
      forecasts[,i]=modelo$coefficients[1]+modelo$coefficients[2]*z[j[,i],2]
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==0 & comp[2]==0 & comp[3]==1){
      modelo<-lm(data$y ~ data$x3)
      forecasts[,i]=modelo$coefficients[1]+modelo$coefficients[2]*z[j[,i],2]
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==0 & comp[2]==0 & comp[3]==0){
      print(modelo<-"ERRORRR !!!")
    }
  }
  

  forecast=as.numeric(forecasts) ### La matriz de pronosticos conviertala en vector
  forecast[forecast<0]=0 ## Todos los pronosticos negativos son iguales a 0 por definición de la precipitación.
  
  
  # se hace un data frame con los datos observados y los prontosicados
  cv<-cbind.data.frame(obs=results$ind[1:30], forecasts= forecast)
  
  # se calcula el goodness index que corresponde al coeficiente de correlación
  # de Pearson
  kendall<-cor(results$ind[1:30], forecast, method = "kendall")
  
  # Realice una lista donde se almacenan todos los resultados
  resultados=list(cv=cv, kendall=kendall, coeficientes=coef) ## Almacene los pronosticos y los resumenes de los modelos en una lista

  # Directorio en el cual se guardan los archivos. 
  setwd(paste("C:/Users/AESQUIVEL/Google Drive/salidas_afm/results_graphs/cv/", answer, "/",sep=""))
  
  # Almacene un archivo con la validación cruzada y otro con los parametros
  write.csv(cv, file = paste("cv_" , answer,"_",a,"_",lead_num,".csv", sep=""))
  write.csv(coef, file = paste("coef_" , answer,"_",a,"_",lead_num,".csv", sep=""))
  
  # retorne la lista de resultados.
  return(resultados)}


# Lea la tabla donde se tienen las coordenadas de las regiones predictoras
tabla=read.table("clipboard",header = T)
rownames(tabla)<-paste(tabla[,"Station"],tabla[,"Trimestre" ], sep="_")

# Lea la tabla en la cual tiene almacenados los modelos que se correran 
# En esta tabla se tiene la información de cuales componentes ingresaran al modelo
tab_comp<-read.table("clipboard",header = T)
rownames(tab_comp)<-paste(substring(tab_comp[,2],1,5),tab_comp[,1], tab_comp[,3], sep="_")

# Declare las variables de la función 
dep<-"santander"
answer<-"StaIsabel"
lead<-tab_comp[,"lead"]
lead_num<-tab_comp[,"lead_num"]
a<-tab_comp[,"a"]
num_zone<-tab_comp[,"num_reg"]
comp<-tab_comp[,c("cor1","cor2", "cor3")]


# Vuelva al espacio de trabajo original 
setwd("C:/Users/AESQUIVEL/Google Drive/salidas_afm")
getwd()

# Corra la función y almacene el goodness index
GI<-0
mo_p<-0
for(i in 1:dim(tab_comp)[1]){
  GI[i]<-cross_validation(dep,lead[i],lead_num[i], a[i], answer, num_zone[i], comp[i,])$kendall
  data<-cbind.data.frame(dep, answer,a[i], lead_num[i], num_zone[i],GI[i])
  mo_p=rbind(mo_p, data)  
}

#mop<-rbind(mop,mo_p[-1, ])

# Almacene el archivo
write.csv(mop, file = "summary_models_parcial_Sant.csv", row.names = TRUE)











### Validación Retrospectiva

## Esta función realiza la validación retrospectiva del modelo
## Elimina un periodo de años y luego pronostica el siguiente
## es decir, incialmente toma como periodo de entrenamiento desde 1982 hasta 2005 para pronosticar 2006
## despues ajusta el modelo con el periodo 1982 a 2006 para pronosticar 2007, así sucesivamente
## hasta llegar a 2013. 

## dep = departamento donde se encuentra la estación 
## lead = lead time con el que se pronostica ("MAM", "Feb")
## lead_num = lead time con el que se pronostica ("Sim", "0", "3")
## a = mes de incio del trimestre
## answer = estación variable de respuesta (ej = "Turipana", "Nataima", ...)
## num_zone = número de zonas con el que se realiza el AFM
## comp = vector de tamaño 3 de 0 y 1 (1 indica que esa componente va a ser utilizada en el modelo)
restrospective_validation<-function(dep,lead,lead_num, a, answer, num_zone, comp){
  
  results<-correlation(dep, lead, a,answer) # corre lo mapas de correlaciones y devuelve
  # la información de la estación y la sst (trimestral)
  MFA<-MFA_P(dep, a, lead, num_zone, results, answer) ## Corre la información del AFM
  
  # Condiciona la cantidad de pronosticos de acuerdo al trimestre
  # puesto que DEF posee menos observaciones, el año de referencia se toma como año al 
  # que pertenece el mes central del periodo. 
  if(a==12){
    length_r=2013-2005 # número de años a pronosticar
    year<-1983:2013 # años a pronosticar
  } else if(a == 3 | a == 6 | a == 9) {
    length_r=2013-2005
    year<-1982:2013
  } 

  # Crea una matriz para almacenar los pronosticos. 
  forecasts=matrix(which(year>2005),length_r,1, byrow = F) ## Matriz en blanco de pronostico
  pron_year<-2005:2013
  
  
  # Crea un data frame con los resultados y las componentes 
  z<-cbind.data.frame( year, y=results$ind, x1=MFA$res_global.pca[,1], x2=MFA$res_global.pca[,2], x3=MFA$res_global.pca[,3])
  
  # crea una matriz para almacenar los coeficientes de regresión de todas las iteraciones
  coef<-matrix(c(1:(sum(comp)+1)),length_r,(sum(comp)+1), byrow = T)
  
  
  
  # Realiza la validación retrospectiva
  for(i in 1:length_r){ # repita esto, de acuerdo a la cantidad de prontosicos
    # cree una matriz j, que contiene solo las posiciones que se deben de eliminar del analisis
    # por ejemplo en la primera interación se debe de eliminar desde 2006 a 2013
    # mientras en la siguiente 2007 a 2013 ... y en la última solo se debe eliminar 2013
    j=matrix(which(year>pron_year[i]),length(which(year>pron_year[i])),1, byrow = F) ## Matriz de posiciones
    data<-z[-j,] # Elimine las observaciones de j, de la trama de datos
    eval<-z[forecasts[i],] # Evalue solo la posición que se va a prontosicar (año objetivo)
    # condicione los análisis de acuerdo a las componentes que entren al modelo
    # una vez calculado el modelo pronostique el año objetivo
    # guarde los coefientes de regresión de cada iteración 
     if(comp[1]==1 & comp[2]==1 & comp[3]==1){
      modelo<-lm(data$y ~ data$x1 + data$x2 + data$x3)
      forecasts[i,]=modelo$coefficients[1]+modelo$coefficients[2]*eval$x1+modelo$coefficients[3]*eval$x2+modelo$coefficients[4]*eval$x3
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==1 & comp[2]==1 & comp[3]==0){
      modelo<-lm(data$y ~data$x1 + data$x2)
      forecasts[i,]=modelo$coefficients[1]+modelo$coefficients[2]*eval$x1+modelo$coefficients[3]*eval$x2
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==1 & comp[2]==0 & comp[3]==1){
      modelo<-lm( data$y ~data$x1 + data$x3)
      forecasts[i,]=modelo$coefficients[1]+modelo$coefficients[2]*eval$x1+modelo$coefficients[3]*eval$x3
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==1 & comp[2]==0 & comp[3]==0){
      modelo<-lm(data$y ~data$x1)
      forecasts[i,]=modelo$coefficients[1]+modelo$coefficients[2]*eval$x1
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==0 & comp[2]==1 & comp[3]==1){
      modelo<-lm(data$y ~ data$x2 + data$x3)
      forecasts[i,]=modelo$coefficients[1]+modelo$coefficients[2]*eval$x2+modelo$coefficients[3]*eval$x3
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==0 & comp[2]==1 & comp[3]==0){
      modelo<-lm(data$y ~ data$x2)
      forecasts[i,]=modelo$coefficients[1]+modelo$coefficients[2]*eval$x2
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==0 & comp[2]==0 & comp[3]==1){
      modelo<-lm(data$y ~ data$x3)
      forecasts[i,]=modelo$coefficients[1]+modelo$coefficients[2]*eval$x3
      coef[i,]<-modelo$coefficients
    }else if(comp[1]==0 & comp[2]==0 & comp[3]==0){
      print(modelo<-"ERRORRR !!!")
    }
  }
  
  forecasts=as.numeric(forecasts) ### La matriz de pronosticos conviertala en vector
  forecasts[forecasts<0]=0 ## Todos los pronosticos negativos son iguales a 0 por definición de la precipitación.
  
  # cree una trama de datos con el año, la observación y lo pronosticado
  retro<-cbind.data.frame(year =z[which(year>2005),1] , obs=z[which(year>2005),2], forecasts= forecasts)
  
  # calcule el goodness index
  kendall<-cor(z[which(year>2005),2], forecasts, method = "kendall")
  
  # cree una lista con los resultados
  resultados=list(kendall=kendall, retro= retro, coeficientes=coef) ## Almacene lso pronosticos y los resumenes de los modelos en una lista

  # cambie el directorio donde se guardaran los archivos.
  setwd(paste("C:/Users/AESQUIVEL/Google Drive/salidas_afm/results_graphs/retro/", answer, "/",sep=""))
  
  write.csv( retro, file = paste(" retro_" , answer,"_",a,"_",lead_num,".csv", sep=""))
  write.csv(coef, file = paste("coef_" , answer,"_",a,"_",lead_num,".csv", sep=""))
  
  # Retorne la lista de resultados.
  return(resultados)}


# Lea la tabla donde se tienen las coordenadas de las regiones predictoras
tabla=read.table("clipboard",header = T)
rownames(tabla)<-paste(tabla[,"Station"],tabla[,"Trimestre" ], sep="_")

# Lea la tabla en la cual tiene almacenados los modelos que se correran 
# En esta tabla se tiene la información de cuales componentes ingresaran al modelo
tab_comp<-read.table("clipboard",header = T)
rownames(tab_comp)<-paste(substring(tab_comp[,2],1,5),tab_comp[,1], tab_comp[,3], sep="_")

# Declare las variables
dep<-"santander"
answer<-"StaIsabel"
lead<-tab_comp[,"lead"]
lead_num<-tab_comp[,"lead_num"]
a<-tab_comp[,"a"]
num_zone<-tab_comp[,"num_reg"]
comp<-tab_comp[,c("cor1","cor2", "cor3")]

# Corra la función y almacene el goodness index
retro<-0
mo_p<-0
for(i in 1:dim(tab_comp)[1]){
  retro[i]<-restrospective_validation(dep,lead[i],lead_num[i], a[i], answer, num_zone[i], comp[i,])$kendall
  data<-cbind.data.frame(dep, answer,a[i], lead_num[i], num_zone[i],retro[i])
  mo_p=rbind(mo_p, data)  
  
}

mop_retro<-rbind(mop_retro,mo_p[-1,])

## Cambie el directorio y almacene los archivos. 
setwd("C:/Users/AESQUIVEL/Google Drive/salidas_afm/")
getwd()
write.csv(mop_retro, file = "summary_models_retro_Sant.csv", row.names = TRUE)


#RMSE <- sqrt(sum((modelo$fitted.values-results$ind)^2)/length(results$ind))











































###############################################################
###############################################################
###############################################################
###############################################################

###   Gráfico


tab<-read.table("clipboard",header = T) # Lea la tabla que tiene almacenada las validaciones
tab[which(tab[,"a"]==12),"a"]=0 # al trimestre 12 (DEF) asignele el valor de 0 para que aparezca 
# primero en los gráficos. 


# declare las etiquetas para el gráfico, labels es para los triemstres
labels<-as_labeller(c("0"="DEF","3"="MAM","6"="JJA", "9"="SON"))
# labels_e es para cambiar el nombre las estaciones a los sitios de interes. 
labels_e<-as_labeller(c("AptoYopal"="Yopal","DoctrinaLa"="Lorica","Turipana" ="Cereté", "AptoPerales"="Ibagué", "Nataima" = "Espinal", "CentAdmoLaUnion"="La Unión", "StaIsabel"="Villanueva"))

# Gráfique los GI condicionando por sitio de estudio y trimestre
ggplot(tab,aes(x=good,y=retro,shape=lead_num,color=lead_num, size=0.2))+geom_point()+
  facet_grid(a~answer,  labeller = labeller(a = labels, answer=labels_e))+theme_bw()+
  scale_size(guide=F)+scale_shape(name="Lead Time")+ylab("Goodness Index Retro")+
  xlab("Goodness Index CV")+
  theme(strip.text.x = element_text(size = 11)) + 
  geom_hline(yintercept = 0.3, colour = "black", linetype = "dotted") + 
  geom_vline(xintercept = 0.3, colour = "black", linetype = "dotted") 

# guarde el gráfico. 
getwd()
ggsave("best_model.png",width =12 ,height =6,dpi=200 )












#### En esta sección se encuentra como correr los ACP en los casos donde se tenga una 
#### sola región predictora 

#### Para la región de Diego, (se realizaría un ACP)
region<-extent(180,252,-13,9); region=crop(results$SST, region)
plot(region)

pca<-dudi.pca(na.omit(region[]),scan = FALSE)
pca$co
#s.corcircle(pca$co)
#biplot(pca)

cor(results$ind, pca$co[,1]) # no da buenos resultados
cor(results$ind, pca$co[,2]) # no da buenos resultados




### Para toda la sst
plot(results$SST)
pca<-dudi.pca(na.omit(results$SST[]),scan = FALSE)
pca$co
cor(results$ind, pca$co[,1]) # no da buenos resultados
cor(results$ind, pca$co[,2]) # no da buenos resultados





























tabla=read.table("clipboard",header = T)

corr_cp1<-ggplot(tabla, aes(x=ev, cor_dim1, colour=Method, shape=resp ))+ geom_point(size=3)
corr_cp1<-corr_cp1 + theme_bw()  + theme(axis.text.x = element_text(angle = -90, hjust = 1))
corr_cp1<-corr_cp1 + geom_hline(yintercept =0.3)

corr_cp2<-ggplot(tabla, aes(x=ev, cor_dim2, colour=Method, shape=resp ))+ geom_point(size=3)
corr_cp2<-corr_cp2 + theme_bw()  + theme(axis.text.x = element_text(angle = -90, hjust = 1)) + ylim(min(tabla$cor_dim1), max(tabla$cor_dim1))
corr_cp2<-corr_cp2+ geom_hline(yintercept =0.3)

grid.arrange(corr_cp1,corr_cp2,ncol=2)





ggplot(na.omit(tabla), aes(x=resp, y= R2_adj, fill=Method)) + geom_bar(stat = "identity") + 
  theme_bw()   +  facet_wrap(~ev, nrow=1)+ theme(axis.text.x = element_text(angle = -90, hjust = 1)) 



ggplot(tabla, aes(x=ev, RMSE, colour=Method, shape=resp))+ geom_point(size=3) + coord_flip()+ theme_bw()  + theme(axis.text.x = element_text(angle = -90, hjust = 1))


ggplot(tabla, aes(ev,p, fill=resp)) + geom_bar(stat = "identity")




